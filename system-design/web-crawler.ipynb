{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a7554c-4f99-4a2a-a2f7-9153ab0f0e88",
   "metadata": {},
   "source": [
    "# Design Web Crawler\n",
    "\n",
    "## Functional\n",
    "- Crawl all the web. (HTML only)\n",
    "- Consider HTTP protocol only.\n",
    "\n",
    "## Capacity\n",
    "\n",
    "### Assume\n",
    "- 1 billion web sites, which is in turn 15 billion HTML pages.\n",
    "- Then, 15B / (4 weeks * 7 days * 86400 sec) = 6200 pages/sec.\n",
    "- Page size is 100KB on average with 500 bytes metadata.\n",
    "\n",
    "### Storage\n",
    "- 15B * (100KB + 500) = 1.5 petabytes.\n",
    "\n",
    "## Design\n",
    "\n",
    "### In each iteration\n",
    "- Pick a URL from unvisited URL list.\n",
    "- Determine the ID address of the hostname.\n",
    "- Download the document.\n",
    "- Parse document contents to look for new URLs.\n",
    "- Add new URLs to unvisited URL list.\n",
    "- Process downloaded document.\n",
    "\n",
    "<img src=\"img/web-crawler1.png\" style=\"width:800px;height:600px;\">\n",
    "\n",
    "### URL frontier\n",
    "- Contains all the remaining URLs to download.\n",
    "- Prioritize which URLs should be downloaded first.\n",
    "- Use BFS implemented by a queue.\n",
    "- Distributed into multiple servers.\n",
    "    - Each server maintains many queues such that each thread gets a queue.\n",
    "    - Hash table maps carnonical hostname to thread number that run the queue.\n",
    "- Due to huge size of URLs, need to store URLs into a disk.\n",
    "    - Enqueue buffer, once filled, is dumped into disk.\n",
    "    - Dequeue buffer keeps cache of URLs to be visited. It periodically reads from disk to fill the buffer.\n",
    "    \n",
    "### HTML fetcher\n",
    "- Downloads documents corresponding to givne URL using protocol like HTTP.\n",
    "\n",
    "### Document input stream\n",
    "- Caches the entire document contents.\n",
    "- Prevents downloading the same document again.\n",
    "\n",
    "### Document de-dup\n",
    "- Checks if document is duplicate.\n",
    "- Calculates 64-bit checksum of every processed document using MD5 or SHA and store it in DB.\n",
    "\n",
    "### URL filter\n",
    "- Blocks some websites so that crawler can ignore them.\n",
    "\n",
    "### DNS resolver\n",
    "- Caches DNS lookup results.\n",
    "\n",
    "### URL de-dup\n",
    "- Check if URL is duplicates. (Multiple URL linking to the same document)\n",
    "- Store checksum of URLs in carnonical forms into DB.\n",
    "\n",
    "## Fault tolerance\n",
    "- Consistent hashing\n",
    "\n",
    "## Data partitioning\n",
    "- Distribute based on hostname which contains\n",
    "    - URLs to visit.\n",
    "    - URL checksum.\n",
    "    - Document checksum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504d1f7c-9787-47d2-b6bc-a9b7400a9c23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
